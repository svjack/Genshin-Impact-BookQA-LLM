import ollama
ollama.pull('qwen:7b')

from gradio_client import Client
client = Client("https://svjack-entity-property-extractor-zh.hf.space")

import pandas as pd
import numpy as np
import os
import re

from langchain.vectorstores import FAISS
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain import chains
from rapidfuzz import fuzz

import pandas as pd
from ollama import AsyncClient

from huggingface_hub import snapshot_download

if not os.path.exists("genshin_book_chunks_with_qa_sp"):
    path = snapshot_download(
        repo_id="svjack/genshin_book_chunks_with_qa_sp",
        repo_type="dataset",
        local_dir="genshin_book_chunks_with_qa_sp",
        local_dir_use_symlinks = False
    )

if not os.path.exists("bge_small_book_chunks_prebuld"):
    path = snapshot_download(
        repo_id="svjack/bge_small_book_chunks_prebuld",
        repo_type="dataset",
        local_dir="bge_small_book_chunks_prebuld",
        local_dir_use_symlinks = False
    )


async def chat_messages(messages, model_name = "gemma:7b", max_length = 128, show_process = False):
    #message = {'role': 'user', 'content': prompt}
    assert type(messages) == type([])
    req = ""
    async for part in await AsyncClient().chat(model=model_name, messages=messages, stream=True):
        if show_process:
            print(part['message']['content'], end='', flush=True)
        req += part['message']['content']
        if len(req) >= max_length:
            break
    return req


'''
query = "è­¦å¯Ÿæ˜¯å¦‚ä½•ç ´è·é‚ªæ¶è®¡åˆ’çš„ï¼Ÿ" ## è­¦ æ‰§å¾‹ ç›—
k = 10
uniform_recall_docs_to_pairwise_cos(
    query,
    docsearch_bge_loaded.similarity_search_with_score(query, k = k, ),
    bge_book_embeddings
)
'''
def uniform_recall_docs_to_pairwise_cos(query ,doc_list, embeddings):
    assert type(doc_list) == type([])
    from langchain.evaluation import load_evaluator
    from langchain.evaluation import EmbeddingDistance
    hf_evaluator = load_evaluator("pairwise_embedding_distance", embeddings=embeddings,
                              distance_metric = EmbeddingDistance.COSINE)
    return sorted(pd.Series(doc_list).map(lambda x: x[0].page_content).map(lambda x:
        (x ,hf_evaluator.evaluate_string_pairs(prediction=query, prediction_b=x)["score"])
    ).values.tolist(), key = lambda t2: t2[1])

'''
sort_by_kw("æ·±æ¸Šä½¿å¾’", book_df)["content_chunks_formatted"].head(5).values.tolist() ### æ·±æ¸Š
'''
def sort_by_kw(kw, book_df):
    req = book_df.copy()
    req["sim_score"] = req.apply(
    lambda x:
    max(map(lambda y: fuzz.ratio(y, kw) ,eval(x["person"]) + eval(x["locate"]) + eval(x["locate"]))) if \
    eval(x["person"]) + eval(x["locate"]) + eval(x["locate"]) else 0
    , axis = 1
)
    req = req.sort_values(by = "sim_score", ascending = False)
    return req

def recall_chuncks(query, docsearch, embedding, book_df,
    sparse_threshold = 30,
    dense_top_k = 10,
    rerank_by = "emb",
):
    sparse_output = sort_by_kw(query, book_df)[["content_chunks_formatted", "sim_score"]]
    sparse_output_list = sparse_output[
        sparse_output["sim_score"] >= sparse_threshold
    ]["content_chunks_formatted"].values.tolist()
    dense_output = uniform_recall_docs_to_pairwise_cos(
        query,
        docsearch.similarity_search_with_score(query, k = dense_top_k,),
        embedding
    )
    for chunck, score in dense_output:
        if chunck not in sparse_output_list:
            sparse_output_list.append(chunck)
    if rerank_by == "emb":
        from langchain.evaluation import load_evaluator
        from langchain.evaluation import EmbeddingDistance
        hf_evaluator = load_evaluator("pairwise_embedding_distance", embeddings=embedding,
                                  distance_metric = EmbeddingDistance.COSINE)
        return pd.Series(sorted(pd.Series(sparse_output_list).map(lambda x:
            (x ,hf_evaluator.evaluate_string_pairs(prediction=query, prediction_b=x)["score"])
        ).values.tolist(), key = lambda t2: t2[1])).map(lambda x: x[0]).values.tolist()
    else:
        sparse_output_list = sorted(sparse_output_list, key = lambda x: fuzz.ratio(x, query), reverse = True)
    return sparse_output_list

def reduce_list_by_order(text_list, as_text = False):
    if not text_list:
        return
    df = pd.DataFrame(
        list(map(lambda x: (x.split("\n")[0], x.split("\n")[1], "\n".join(x.split("\n")[2:])), text_list))
    ).groupby([0, 1])[2].apply(list).map(lambda x: sorted(x, key = len, reverse=True)).map(
        "\n\n".join
    ).reset_index()
    d = dict(df.apply(lambda x: ((x.iloc[0], x.iloc[1]), x.iloc[2]), axis = 1).values.tolist())
    #return df
    order_list = []
    for x in text_list:
        a, b = x.split("\n")[0], x.split("\n")[1]
        if not order_list:
            order_list = [[a, [b]]]
        elif a in list(map(lambda t2: t2[0], order_list)):
            order_list[list(map(lambda t2: t2[0], order_list)).index(a)][1].append(b)
        elif a not in list(map(lambda t2: t2[0], order_list)):
            order_list.append([a, [b]])
    df = pd.DataFrame(pd.DataFrame(order_list).explode(1).dropna().apply(
        lambda x: (x.iloc[0], x.iloc[1], d[(x.iloc[0], x.iloc[1])]), axis = 1
    ).values.tolist()).drop_duplicates()
    if as_text:
        return "\n\n".join(
            df.apply(lambda x: "{}\n{}\n{}".format(x.iloc[0], x.iloc[1], x.iloc[2]), axis = 1).values.tolist()
        )
    return df

def build_gpt_prompt(query, docsearch, embedding, book_df, max_context_length = 4090):
    l = recall_chuncks(query, docsearch, embedding, book_df)
    context = reduce_list_by_order(l, as_text = True)
    context_l = []
    for ele in context.split("\n"):
        if sum(map(len, context_l)) >= max_context_length:
            break
        context_l.append(ele)
    context = "\n".join(context_l).strip()
    template = """ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”æ¡ˆã€‚å°½é‡ä½¿ç­”æ¡ˆç®€æ˜æ‰¼è¦ã€‚æ€»æ˜¯åœ¨å›ç­”çš„æœ€åè¯´â€œè°¢è°¢ä½ çš„æé—®ï¼â€ã€‚

{context}

é—®é¢˜: {question}
æœ‰ç”¨çš„å›ç­”:"""
    return template.format(
        **{
            "context": context,
            "question": query
        }
    )

def collect_prompt_to_hist_list(prompt, add_assistant = False):
    l = pd.Series(prompt.split("\n\n")).map(lambda x: x.strip()).values.tolist()
    ll = []
    for ele in l:
        if not ll:
            ll.append(ele)
        else:
            if ele.startswith("æ–‡ç« æ ‡é¢˜ï¼š") or ele.startswith("é—®é¢˜:"):
                ll.append(ele)
            else:
                ll[-1] += ("\n\n" + ele)
    if add_assistant:
        ll_ = []
        for i in range(len(ll)):
            if i == 0:
                ll_.append((ll[i], "å¥½çš„ã€‚"))
            elif i < len(ll) - 1:
                ll_.append((ll[i], "æˆ‘è¯»æ‡‚äº†ã€‚"))
            else:
                ll_.append((ll[i], ""))
        return ll_
    else:
        return ll

def row_to_content_ask(r):
    question = r["question"]
    content_list = r["content_list"]
    assert type(content_list) == type([])
    content_prompt_list = pd.Series(content_list).map(
        lambda x:  '''
    {}\nä»ä¸Šé¢çš„ç›¸å…³çš„å™è¿°ä¸­æŠ½å–åŒ…å«"{}"ä¸­è¯æ±‡çš„ç›¸å…³è¯­æ®µã€‚
    '''.format(x, question).strip()
    ).values.tolist()
    return content_prompt_list

async def entity_extractor_by_ollama(query,
    model_name = "gemma:7b", show_process = False, max_length = 512,
    return_out_text = False,
    ):
    import re
    hist = [
    ['è¯·ä»ä¸‹é¢çš„å¥å­ä¸­æå–å®ä½“å’Œå±æ€§ã€‚ä¸éœ€è¦è¿›è¡Œè¿›ä¸€æ­¥è§£é‡Šã€‚', 'å¥½çš„ã€‚'],
     ['é—®é¢˜ï¼šå®æ³¢åœ¨å“ªä¸ªçœä»½ï¼Ÿ', 'å®ä½“ï¼šå®æ³¢ å±æ€§ï¼šçœä»½'],
     ['é—®é¢˜ï¼šä¸­å›½çš„è´§å¸æ˜¯ä»€ä¹ˆï¼Ÿ', 'å®ä½“ï¼šä¸­å›½ å±æ€§ï¼šè´§å¸'],
     ['é—®é¢˜ï¼šç™¾æ…•å¤§ä¸‰è§’åœ¨ä»€ä¹ˆåœ°æ–¹ï¼Ÿ', 'å®ä½“ï¼šç™¾æ…•å¤§ä¸‰è§’ å±æ€§ï¼šåœ°æ–¹'],
     ['é—®é¢˜ï¼šè°æ˜¯æœ€å¯çˆ±çš„äººï¼Ÿ', "å®ä½“ï¼šäºº å±æ€§ï¼šå¯çˆ±"],
     ['é—®é¢˜ï¼šé»„æ²³çš„æ‹ç‚¹åœ¨å“ªé‡Œï¼Ÿ', "å®ä½“ï¼šé»„æ²³ å±æ€§ï¼šæ‹ç‚¹"],
     #['é—®é¢˜ï¼šé­”ç¥å½’ç»ˆåœ¨å“ªé‡Œï¼Ÿ', 'å®ä½“ï¼šå½’ç»ˆ å±æ€§ï¼šå“ªé‡Œ'],
     #["ç‰ç±³çš„å¼•è¿›æ—¶é—´æ˜¯ä»€ä¹ˆæ—¶å€™ï¼Ÿ", ""]
     ]

    re_hist = pd.DataFrame(
    pd.Series(hist).map(
        lambda t2: (
            {
                "role": "user",
                "content": t2[0]
            },
            {
                "role": "assistant",
                "content": t2[1]
            },
        )
    ).values.tolist()).values.reshape([-1]).tolist()

    out_text = await chat_messages(re_hist + [{"role": "user", "content": "é—®é¢˜ï¼š{}".format(query)}],
        model_name, show_process = show_process)
    if return_out_text:
        return out_text
    e_list = re.findall(r"å®ä½“(.*?)å±æ€§", out_text.replace("\n", " "))
    if e_list:
        return re.findall(u"[\u4e00-\u9fa5]+" ,e_list[0])
    return None

def unzip_string(x, size = 2):
    if len(x) <= size:
        return [x]
    req = []
    for i in range(len(x) - size + 1):
        req.append(x[i: i + size])
    return req

def entity_extractor_by_adapter(x):
    import json
    result = client.predict(
    		x,	# str  in 'question' Textbox component
    		api_name="/predict"
    )
    with open(result, "r") as f:
        req = json.load(f)
    req_list = req.get("E-TAG", [])
    req_ = []
    for ele in req_list:
        for x in unzip_string(ele, 2):
            if x not in req_:
                req_.append(x)
    return req_

async def query_content_ask_func(question, content_list,
        model_name, setfit_model, show_process = False, max_length = 1024):
    ask_list = row_to_content_ask(
        {
            "question": question,
            "content_list": content_list
        }
    )
    #return ask_list
    req = []
    for prompt in ask_list:
        out_text = await chat_messages([] + [{"role": "user", "content":
            prompt + "å¦‚æœæ²¡æœ‰æåˆ°ç›¸å…³å†…å®¹ï¼Œè¯·å›ç­”ä¸çŸ¥é“ã€‚ä½¿ç”¨ä¸­æ–‡è¿›è¡Œå›ç­”ï¼Œä¸è¦åŒ…å«ä»»ä½•è‹±æ–‡ã€‚"
        }],
            model_name, show_process = show_process, max_length = max_length)
        req.append(out_text)
    d = {
            "question": question,
            "content_list": content_list
        }
    assert len(req) == len(ask_list)
    d["question_content_relate_list"] = req
    d["relate_prob_list"] = setfit_model.predict_proba(
               req
        ).numpy()[:, 1].tolist()
    return d

async def build_relate_ask_list(query, docsearch_bge_loaded, bge_book_embeddings, book_df,
                          relate_model_name,
                          et_model_name,
                          setfit_model, as_content_score_df = True,
                          show_process = False, add_relate_entities = False,
                          max_length = 1024):
    prompt = build_gpt_prompt(query, docsearch_bge_loaded, bge_book_embeddings, book_df)
    prompt_list = collect_prompt_to_hist_list(prompt)
    question = prompt_list[-1].split("\n")[0]
    content_list = prompt_list[1:-1]

    d = await query_content_ask_func(question, content_list,
            relate_model_name, setfit_model, show_process = show_process)

    #entity_list = await entity_extractor_by_ollama(query, et_model_name, show_process = show_process, max_length = max_length)
    entity_list = entity_extractor_by_adapter(query)
    if type(entity_list) != type([]):
        entity_list = []

    d["in_content_entity_list"] = list(map(lambda x:
        list(filter(lambda e: e in x, entity_list))
    , d["content_list"]))

    if add_relate_entities:
        relate_content_entity_list = [[]] * len(content_list)

        for entity in entity_list:
            entity_content_score_d = await query_content_ask_func(entity, d["content_list"],
            relate_model_name, setfit_model, show_process = show_process)
            lookup_df = pd.DataFrame(
                list(zip(*[entity_content_score_d["content_list"],
                entity_content_score_d["relate_prob_list"]]))
            )
            for ii, (i, r) in enumerate(lookup_df.iterrows()):
                if r.iloc[1] >= 0.5 and entity not in relate_content_entity_list[ii]:
                    #relate_content_entity_list[ii].append(entity)
                    relate_content_entity_list[ii] = relate_content_entity_list[ii] + [entity]

        d["relate_content_entity_list"] = relate_content_entity_list

    if as_content_score_df:
        if add_relate_entities:
            df = pd.concat(
                [
                    pd.Series(d["content_list"]).map(lambda x: x.strip()),
                    pd.Series(d["in_content_entity_list"]),
                    pd.Series(d["relate_content_entity_list"]),
                    pd.Series(d["question_content_relate_list"]).map(lambda x: x.strip()),
                    pd.Series(d["relate_prob_list"])
                ], axis = 1
            )
            df.columns = ["content", "entities", "relate_entities", "relate_eval_str", "score"]
        else:
            df = pd.concat(
                [
                    pd.Series(d["content_list"]).map(lambda x: x.strip()),
                    pd.Series(d["in_content_entity_list"]),
                    #pd.Series(d["relate_content_entity_list"]),
                    pd.Series(d["question_content_relate_list"]).map(lambda x: x.strip()),
                    pd.Series(d["relate_prob_list"])
                ], axis = 1
            )
            df.columns = ["content", "entities", "relate_eval_str", "score"]
        req = []
        entities_num_list = df["entities"].map(len).drop_duplicates().dropna().sort_values(ascending = False).\
            values.tolist()
        for e_num in entities_num_list:
            req.append(
            df[
                df["entities"].map(lambda x: len(x) == e_num)
            ].sort_values(by = "score", ascending = False)
            )
        return pd.concat(req, axis = 0)
        #df = df.sort_values(by = "score", ascending = False)
        #return df
    return d

async def run_all(query, docsearch_bge_loaded, bge_book_embeddings, book_df,
                    relate_model_name, et_model_name, qa_model_name,
                          setfit_model, only_return_prompt = False):
    df = await build_relate_ask_list(query, docsearch_bge_loaded, bge_book_embeddings, book_df,
                              relate_model_name, et_model_name,
                              setfit_model, show_process=False)
    info_list = df[
        df.apply(
            lambda x: x["score"] >= 0.5 and bool(x["entities"]), axis = 1
        )
    ].values.tolist()
    if not info_list:
        return df, info_list, "æ²¡æœ‰ç›¸å…³å†…å®¹ï¼Œè°¢è°¢ä½ çš„æé—®ã€‚"
    prompt = '''
é—®é¢˜: {}
æ ¹æ®ä¸‹é¢çš„å†…å®¹å›ç­”ä¸Šé¢çš„é—®é¢˜ï¼Œå¦‚æœæ— æ³•æ ¹æ®å†…å®¹ç¡®å®šç­”æ¡ˆï¼Œè¯·å›ç­”ä¸çŸ¥é“ã€‚
{}
'''.format(query, "\n\n".join(pd.Series(info_list).map(lambda x: x[0]).values.tolist()))
    if only_return_prompt:
        return df, info_list, prompt

    q_head = "\n".join(prompt.split("\n")[:2])
    c_tail = "\n".join(prompt.split("\n")[2:])[:4000]
    out_text = await chat_messages([] + [{"role": "user", "content":
        c_tail + "\n" + q_head.replace("ä¸‹é¢çš„å†…å®¹å›ç­”ä¸Šé¢çš„é—®é¢˜", "ä¸Šé¢çš„å†…å®¹å›ç­”é—®é¢˜") + "ç”¨ä¸­æ–‡å›ç­”é—®é¢˜ã€‚",
    }],
        qa_model_name, show_process = False, max_length = 512)
    #out = mistral_predict(prompt + "\nä½¿ç”¨ä¸­æ–‡è¿›è¡Œå›ç­”ï¼Œä¸è¦åŒ…å«ä»»ä½•è‹±æ–‡ã€‚", llm)
    return df, info_list, out_text

#book_df = pd.read_csv("genshin_book_chunks_with_qa_sp.csv")
book_df = pd.read_csv("genshin_book_chunks_with_qa_sp/genshin_book_chunks_with_qa_sp.csv")
book_df["content_chunks"].dropna().drop_duplicates().shape

book_df["content_chunks_formatted"] = book_df.apply(
        lambda x: "æ–‡ç« æ ‡é¢˜ï¼š{}\nå­æ ‡é¢˜ï¼š{}\nå†…å®¹ï¼š{}".format(x["title"], x["sub_title"], x["content_chunks"]),
        axis = 1
    )

texts = book_df["content_chunks_formatted"].dropna().drop_duplicates().values.tolist()

#embedding_path = "bge-small-book-qa/"
embedding_path = "svjack/bge-small-book-qa"
bge_book_embeddings = HuggingFaceEmbeddings(model_name=embedding_path)
docsearch_bge_loaded = FAISS.load_local("bge_small_book_chunks_prebuld/", bge_book_embeddings)

from setfit import SetFitModel
#setfit_model = SetFitModel.from_pretrained("setfit_info_cls")
setfit_model = SetFitModel.from_pretrained("svjack/setfit_info_cls")

import gradio as gr

with gr.Blocks() as demo:
    title = gr.HTML(
            """<h1 align="center"> <font size="+3"> Genshin Impact Book QA Qwen-7B Demo ğŸ” </font> </h1>""",
            elem_id="title",
    )

    with gr.Column():
        with gr.Row():
            query = gr.Text(label = "è¾“å…¥é—®é¢˜ï¼š", lines = 1, interactive = True, scale = 5.0)
            run_button = gr.Button("å¾—åˆ°ç­”æ¡ˆ")
        output = gr.Text(label = "å›ç­”ï¼š", lines = 5, interactive = True)
        recall_items = gr.JSON(label = "å¬å›ç›¸å…³å†…å®¹", interactive = False)

    with gr.Row():
        gr.Examples(
            [
             'ä¸˜ä¸˜äººæœ‰å“ªäº›ç”Ÿæ´»ä¹ æƒ¯ï¼Ÿ',
             'å²©ç‹å¸å›å’Œå½’ç»ˆæ˜¯ä»€ä¹ˆå…³ç³»ï¼Ÿ',
             'å²©ç‹å¸å›æ˜¯ä¸€ä¸ªä»€ä¹ˆæ ·çš„äººï¼Ÿ',
             'é“³æªæ‰‹çš„æ•…äº‹å†…å®¹æ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿ',
             'çŠç‘šå®«æœ‰å“ªäº›ä¼ è¯´ï¼Ÿ',
             'çµå…‰é¢‚çš„å†…å®¹æ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿ',
             'è¿å¿ƒç è®²äº†ä¸€ä»¶ä»€ä¹ˆäº‹æƒ…ï¼Ÿ',
             'æ¢“å¿ƒæ˜¯è°ï¼Ÿ',
             'ç’ƒæœˆæœ‰å“ªäº›æ•…äº‹ï¼Ÿ',
             'è½»ç­–åº„æœ‰å“ªäº›æ•…äº‹ï¼Ÿ',
             'ç‘¶å…‰æ»©æœ‰å“ªäº›æ•…äº‹ï¼Ÿ',
             'ç¨»å¦»æœ‰å“ªäº›æ•…äº‹ï¼Ÿ',
             'æµ·ç¥‡å²›æœ‰å“ªäº›æ•…äº‹ï¼Ÿ',
             'è’™å¾·æœ‰å“ªäº›æ•…äº‹ï¼Ÿ',
             'ç’ƒæœˆæœ‰å“ªäº›å¥‡çå¼‚å®ï¼Ÿ',
            ],
            inputs = query,
            label = "è¢«ä¹¦ç›®å†…å®¹åŒ…å«çš„é—®é¢˜"
        )
    with gr.Row():
        gr.Examples(
            [
             'çˆ±ä¸½ä¸å¥³å£«æ˜¯å¯è‰çš„å¦ˆå¦ˆå—ï¼Ÿ',
             'æ‘˜æ˜Ÿå´–æ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿ',
             'ä¸˜ä¸˜äººä½¿ç”¨çš„æ˜¯ä»€ä¹ˆæ–‡å­—ï¼Ÿ',
             'æ·±æ¸Šä½¿å¾’å“ªé‡Œæ¥çš„ï¼Ÿ',
             'å‘æ¡æœºå…³å¯ä»¥ç”¨æ¥åšä»€ä¹ˆï¼Ÿ',
             'é‚£å…ˆæœ±é‚£åšäº†ä»€ä¹ˆï¼Ÿ',
            ],
            inputs = query,
            label = "æ²¡æœ‰è¢«ä¹¦ç›®æ˜ç¡®æåˆ°çš„é—®é¢˜"
        )

    async def run_func(x):
        return (await run_all(x, docsearch_bge_loaded, bge_book_embeddings, book_df,
                             et_model_name = "qwen:7b",
                             relate_model_name = "qwen:7b",
                             qa_model_name = "qwen:7b",
                             setfit_model = setfit_model))[1:]

    run_button.click(run_func,
        query, [recall_items, output]
    )

demo.queue(max_size=4, concurrency_count=1).launch(debug=True, show_api=False, share = True)
